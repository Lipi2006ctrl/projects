import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings('ignore')



df = pd.read_csv(r"C:\Users\lipi-\Downloads\housing (3).csv")

print("Shape:", df.shape)
print(df.head())


print("\nDataset info:")
print(df.info())
print("\nMissing values per column:")
print(df.isnull().sum())

possible_targets = ["price", "Price", "SalePrice", "sale_price", "final_price", "FinalPrice"]
target_col = None
for t in possible_targets:
    if t in df.columns:
        target_col = t
        break
if target_col is None:
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    target_col = numeric_cols[-1]

print(f"\n Target column selected: {target_col}")


plt.figure(figsize=(6,4))
sns.histplot(df[target_col], bins=30, kde=True, color='skyblue')
plt.title("Distribution of Target Variable (Price)")
plt.show()


corr = df.select_dtypes(include=[np.number]).corr()
top_corr = corr[target_col].abs().sort_values(ascending=False).head(10).index
plt.figure(figsize=(8,6))
sns.heatmap(df[top_corr].corr(), annot=True, cmap="coolwarm")
plt.title("Top Correlated Features with Target")
plt.show()


df['Price_per_sqft'] = df[target_col] / df['area'] if 'area' in df.columns else np.nan
if 'year_built' in df.columns:
    df['House_Age'] = 2025 - df['year_built']
if {'full_bath', 'half_bath'}.issubset(df.columns):
    df['Total_Bathrooms'] = df['full_bath'] + 0.5 * df['half_bath']
if {'total_rooms', 'area'}.issubset(df.columns):
    df['Rooms_Per_Area'] = df['total_rooms'] / df['area']




Q1 = df[target_col].quantile(0.25)
Q3 = df[target_col].quantile(0.75)
IQR = Q3 - Q1
df = df[(df[target_col] >= Q1 - 1.5*IQR) & (df[target_col] <= Q3 + 1.5*IQR)]
print(f" Outliers removed. New dataset shape: {df.shape}")


X = df.drop(columns=[target_col])
y = np.log1p(df[target_col])  


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


numeric_features = X.select_dtypes(include=[np.number]).columns
categorical_features = X.select_dtypes(exclude=[np.number]).columns

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])


xgb_model = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=7,
    subsample=0.9,
    colsample_bytree=0.9,
    reg_lambda=1,
    reg_alpha=0.1,
    random_state=42
)

pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', xgb_model)])


pipe.fit(X_train, y_train)



y_pred = np.expm1(pipe.predict(X_test))  
y_true = np.expm1(y_test)

r2 = r2_score(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae = mean_absolute_error(y_true, y_pred)

print("\nðŸ“Š Model Performance:")
print(f"RÂ² Score: {r2:.4f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")


scores = cross_val_score(pipe, X, y, cv=5, scoring='r2')
print(f"Cross-Validated RÂ²: {np.mean(scores):.4f}")


xgb = pipe.named_steps['model']
plt.figure(figsize=(10,6))
sns.barplot(x=xgb.feature_importances_, y=range(len(xgb.feature_importances_)))
plt.title("XGBoost Feature Importances (encoded features)")
plt.show()


pred_df = pd.DataFrame({'Actual': y_true, 'Predicted': y_pred})
pred_df.to_csv("predictions.csv", index=False)

