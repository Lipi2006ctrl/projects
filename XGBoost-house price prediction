# =====================================================
# üè† HOUSE PRICE PREDICTION USING XGBOOST
# =====================================================

# STEP 1: Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# STEP 2: Load dataset (upload housing.csv in Colab first)
from google.colab import files
uploaded = files.upload()

df = pd.read_csv("housing.csv")
print("‚úÖ Dataset loaded successfully!")
print("Shape:", df.shape)
df.head()

# STEP 3: Identify target variable
possible_targets = ["price", "Price", "SalePrice", "sale_price", "final_price", "FinalPrice"]
target_col = None
for t in possible_targets:
    if t in df.columns:
        target_col = t
        break

if target_col is None:
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    target_col = numeric_cols[-1]

print("üéØ Target column:", target_col)

# STEP 4: Split features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# STEP 5: Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# STEP 6: Preprocessing pipelines
numeric_features = X.select_dtypes(include=[np.number]).columns
categorical_features = X.select_dtypes(exclude=[np.number]).columns

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# STEP 7: Build XGBoost pipeline
xgb_model = XGBRegressor(
    n_estimators=300,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

pipe = Pipeline(steps=[('preprocessor', preprocessor),
                       ('model', xgb_model)])

# STEP 8: Train model
pipe.fit(X_train, y_train)
print("‚úÖ Model training complete!")

# STEP 9: Evaluate performance
y_pred = pipe.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print(f"üìä R¬≤ Score: {r2:.4f}")
print(f"üìâ RMSE: {rmse:.2f}")
print(f"üí∞ MAE: {mae:.2f}")

# STEP 10: Feature importance
xgb = pipe.named_steps['model']
plt.figure(figsize=(10,6))
sns.barplot(x=xgb.feature_importances_, y=range(len(xgb.feature_importances_)))
plt.title("XGBoost Feature Importances (encoded features)")
plt.show()

# STEP 11: Save predictions
pred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
pred_df.to_csv("predictions.csv", index=False)
print("‚úÖ Predictions saved as 'predictions.csv'")

# STEP 12 (Optional): Hyperparameter tuning
# Uncomment to run (takes longer)
# param_grid = {
#     'model__n_estimators': [200, 300, 400],
#     'model__max_depth': [3, 5, 7],
#     'model__learning_rate': [0.05, 0.1, 0.2]
# }
# grid = GridSearchCV(pipe, param_grid, cv=3, scoring='r2', verbose=2, n_jobs=-1)
# grid.fit(X_train, y_train)
# print("Best parameters:", grid.best_params_)
# print("Best R¬≤:", grid.best_score_)
